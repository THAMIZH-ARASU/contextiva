"""Abstract base interface for LLM and embedding providers.

This module defines the ILLMProvider abstract base class that all concrete
provider implementations must implement. This enables pluggable, runtime-switchable
LLM and embedding providers without coupling to specific vendor APIs.
"""

from abc import ABC, abstractmethod
from typing import AsyncIterator


class ILLMProvider(ABC):
    """Abstract interface for LLM and embedding providers.
    
    All concrete provider implementations (OpenAI, Anthropic, Ollama, etc.)
    must implement this interface to ensure consistent behavior across providers.
    
    The interface supports:
    - Text embeddings generation
    - LLM completions (chat/text generation)
    - Streaming completions for real-time responses
    """

    @abstractmethod
    async def embed_text(self, text: str, model: str | None = None) -> list[float]:
        """Generate embeddings for the given text.
        
        Args:
            text: The text to embed. Should be non-empty.
            model: Optional model name to use. If None, uses the provider's default
                   embedding model configured in settings.
        
        Returns:
            A list of floats representing the embedding vector. The dimension
            depends on the model (e.g., 1536 for OpenAI text-embedding-3-small).
        
        Raises:
            LLMAuthenticationError: If API authentication fails.
            LLMRateLimitError: If the provider's rate limit is exceeded.
            LLMConnectionError: If network connection fails.
            LLMProviderError: For other provider-specific errors.
            NotImplementedError: If the provider does not support embeddings
                                 (e.g., Anthropic, OpenRouter).
        """
        pass

    @abstractmethod
    async def generate_completion(
        self, messages: list[dict], model: str | None = None, **kwargs
    ) -> str:
        """Generate a completion from the LLM.
        
        Args:
            messages: List of message dicts in the format:
                     [{"role": "system"|"user"|"assistant", "content": "..."}]
            model: Optional model name to use. If None, uses the provider's default
                   LLM model configured in settings.
            **kwargs: Additional provider-specific parameters (e.g., temperature,
                     max_tokens, top_p, etc.).
        
        Returns:
            The generated completion text as a string.
        
        Raises:
            LLMAuthenticationError: If API authentication fails.
            LLMRateLimitError: If the provider's rate limit is exceeded.
            LLMConnectionError: If network connection fails.
            LLMProviderError: For other provider-specific errors.
        """
        pass

    @abstractmethod
    async def generate_completion_stream(
        self, messages: list[dict], model: str | None = None, **kwargs
    ) -> AsyncIterator[str]:
        """Generate a streaming completion from the LLM.
        
        This method returns an async iterator that yields completion chunks as
        they are generated by the LLM, enabling real-time streaming responses.
        
        Args:
            messages: List of message dicts in the format:
                     [{"role": "system"|"user"|"assistant", "content": "..."}]
            model: Optional model name to use. If None, uses the provider's default
                   LLM model configured in settings.
            **kwargs: Additional provider-specific parameters (e.g., temperature,
                     max_tokens, top_p, etc.).
        
        Yields:
            Chunks of the completion text as strings.
        
        Raises:
            LLMAuthenticationError: If API authentication fails.
            LLMRateLimitError: If the provider's rate limit is exceeded.
            LLMConnectionError: If network connection fails.
            LLMProviderError: For other provider-specific errors.
        """
        pass
