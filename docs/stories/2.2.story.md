# Story 2.2: LLM & Embedding Provider Factory

## Status

**InProgress** (Implementation started 2025-11-07)

## Story

**As a** AI Agent Developer,
**I want** a flexible, pluggable factory for LLM and embedding providers,
**so that** I am not locked into a single provider and can support local models (NFR8).

## Acceptance Criteria

1. An abstract ILLMProvider interface is defined, specifying methods like `embed_text(...)` and `generate_completion(...)`.
2. Concrete implementations of the interface are created for OpenAIProvider, AnthropicProvider, OllamaProvider, and OpenRouterProvider (NFR8).
3. A ProviderFactory is created that reads the LLM_PROVIDER and EMBEDDING_PROVIDER from settings.py and returns the correct provider instance.
4. The factory is integrated into the FastAPI app's dependency injection system.
5. Configuration in .env.example is updated to include API keys and model names for all supported providers.

## Tasks / Subtasks

- [ ] Create ILLMProvider abstract interface (AC: 1)
  - [ ] Define ILLMProvider ABC in `src/infrastructure/external/llm/providers/base.py` [Source: architecture/source-tree.md#infrastructure-layer]
  - [ ] Add abstract method `async def embed_text(text: str, model: str | None = None) -> list[float]` for generating embeddings
  - [ ] Add abstract method `async def generate_completion(messages: list[dict], model: str | None = None, **kwargs) -> str` for LLM completions
  - [ ] Add abstract method `async def generate_completion_stream(messages: list[dict], model: str | None = None, **kwargs) -> AsyncIterator[str]` for streaming completions
  - [ ] Include docstrings with parameter descriptions and return types [Source: architecture/coding-standards.md]
  - [ ] Add type hints for all methods [Source: architecture/coding-standards.md]

- [ ] Implement OpenAIProvider (AC: 2)
  - [ ] Create OpenAIProvider in `src/infrastructure/external/llm/providers/openai_provider.py` implementing ILLMProvider
  - [ ] Initialize OpenAI client with API key from settings (settings.llm.openai_api_key) [Source: architecture/external-apis.md]
  - [ ] Implement `embed_text()` calling OpenAI embeddings API endpoint `/v1/embeddings` [Source: architecture/external-apis.md]
  - [ ] Default embedding model: "text-embedding-3-small" (1536 dimensions) [Source: architecture/tech-stack.md]
  - [ ] Implement `generate_completion()` calling OpenAI chat completions API `/v1/chat/completions` [Source: architecture/external-apis.md]
  - [ ] Implement `generate_completion_stream()` with streaming support
  - [ ] Add error handling for API rate limits, authentication errors, and network failures
  - [ ] Use httpx for async HTTP requests [Source: architecture/tech-stack.md]

- [ ] Implement AnthropicProvider (AC: 2)
  - [ ] Create AnthropicProvider in `src/infrastructure/external/llm/providers/anthropic_provider.py` implementing ILLMProvider
  - [ ] Initialize Anthropic client with API key from settings (settings.llm.anthropic_api_key) [Source: architecture/external-apis.md]
  - [ ] Implement `embed_text()` - Note: Anthropic does not provide embeddings, delegate to OpenAI or raise NotImplementedError
  - [ ] Implement `generate_completion()` calling Anthropic messages API `/v1/messages` [Source: architecture/external-apis.md]
  - [ ] Default model: "claude-3-haiku-20240307" [Source: architecture/tech-stack.md]
  - [ ] Implement `generate_completion_stream()` with streaming support
  - [ ] Add error handling for rate limits and API errors
  - [ ] Use httpx for async HTTP requests

- [ ] Implement OllamaProvider (AC: 2)
  - [ ] Create OllamaProvider in `src/infrastructure/external/llm/providers/ollama_provider.py` implementing ILLMProvider
  - [ ] Initialize Ollama client with base URL from settings (settings.llm.ollama_base_url, default: http://localhost:11434) [Source: architecture/external-apis.md]
  - [ ] Implement `embed_text()` calling Ollama embeddings API `/api/embeddings`
  - [ ] Implement `generate_completion()` calling Ollama generate API `/api/generate` or `/api/chat` [Source: architecture/external-apis.md]
  - [ ] Implement `generate_completion_stream()` with streaming support
  - [ ] No authentication required for local Ollama [Source: architecture/external-apis.md]
  - [ ] Add connection error handling for when Ollama is not running
  - [ ] Use httpx for async HTTP requests

- [ ] Implement OpenRouterProvider (AC: 2)
  - [ ] Create OpenRouterProvider in `src/infrastructure/external/llm/providers/openrouter_provider.py` implementing ILLMProvider
  - [ ] Initialize OpenRouter client with API key from settings (settings.llm.openrouter_api_key)
  - [ ] Implement `embed_text()` - delegate to configured embedding provider or raise NotImplementedError
  - [ ] Implement `generate_completion()` calling OpenRouter API (OpenAI-compatible endpoint)
  - [ ] Implement `generate_completion_stream()` with streaming support
  - [ ] Add support for 100+ models via model parameter [Source: architecture/tech-stack.md]
  - [ ] Add error handling for rate limits and API errors
  - [ ] Use httpx for async HTTP requests

- [ ] Create ProviderFactory (AC: 3)
  - [ ] Create ProviderFactory in `src/infrastructure/external/llm/provider_factory.py`
  - [ ] Add method `get_llm_provider(provider_name: str | None = None) -> ILLMProvider` that reads from settings.llm.llm_provider
  - [ ] Add method `get_embedding_provider(provider_name: str | None = None) -> ILLMProvider` that reads from settings.llm.embedding_provider
  - [ ] Support provider names: "openai", "anthropic", "ollama", "openrouter" (case-insensitive)
  - [ ] Raise ValueError if unsupported provider is requested
  - [ ] Implement singleton pattern for provider instances to avoid multiple API client initializations
  - [ ] Add logging for provider initialization [Source: architecture/coding-standards.md]

- [ ] Update settings configuration (AC: 5)
  - [ ] Add LLM configuration to `src/shared/config/settings.py`:
    - llm_provider: str (default: "openai")
    - embedding_provider: str (default: "openai")
    - openai_api_key: str | None
    - anthropic_api_key: str | None
    - ollama_base_url: str (default: "http://localhost:11434")
    - openrouter_api_key: str | None
    - default_llm_model: str (default: "gpt-4o-mini")
    - default_embedding_model: str (default: "text-embedding-3-small")
  - [ ] Update `.env.example` with all LLM provider configurations [Source: architecture/external-apis.md]
  - [ ] Add comments in .env.example explaining each provider's purpose

- [ ] Integrate with FastAPI dependency injection (AC: 4)
  - [ ] Create dependency function in `src/api/dependencies.py`: `async def get_llm_provider() -> ILLMProvider`
  - [ ] Create dependency function in `src/api/dependencies.py`: `async def get_embedding_provider() -> ILLMProvider`
  - [ ] Use ProviderFactory to instantiate providers based on settings
  - [ ] Add provider instances to app lifespan context for reuse across requests
  - [ ] Update `src/api/lifespan.py` to initialize providers on startup

- [ ] Unit tests for providers (AC: 1, 2)
  - [ ] Create `tests/unit/infrastructure/external/llm/providers/test_openai_provider.py`
  - [ ] Create `tests/unit/infrastructure/external/llm/providers/test_anthropic_provider.py`
  - [ ] Create `tests/unit/infrastructure/external/llm/providers/test_ollama_provider.py`
  - [ ] Create `tests/unit/infrastructure/external/llm/providers/test_openrouter_provider.py`
  - [ ] Mock external API calls using pytest-httpx [Source: architecture/test-strategy-and-standards.md]
  - [ ] Test successful responses, error handling, rate limits, authentication failures
  - [ ] Follow AAA pattern, aim for 90% coverage [Source: architecture/test-strategy-and-standards.md]

- [ ] Integration tests for ProviderFactory (AC: 3, 4)
  - [ ] Create `tests/integration/infrastructure/external/llm/test_provider_factory.py`
  - [ ] Test factory returns correct provider instance based on settings
  - [ ] Test factory raises ValueError for unsupported provider names
  - [ ] Test singleton pattern (same instance returned on multiple calls)
  - [ ] Test integration with FastAPI dependencies
  - [ ] Use httpx-responses to stub external API endpoints [Source: architecture/test-strategy-and-standards.md]

## Dev Notes

### Architecture Context

**Clean Architecture Compliance**:
- All provider implementations belong to the Infrastructure layer [Source: architecture/components.md]
- ILLMProvider interface is abstract and should have no external dependencies
- Providers implement the interface and can depend on external libraries (openai, anthropic, httpx)
- Application layer will depend on ILLMProvider interface, not concrete implementations [Source: architecture/coding-standards.md]

**Provider Factory Pattern**:
- Factory reads configuration from settings.py (which loads from environment variables) [Source: architecture/coding-standards.md#rule-5]
- Factory returns instances of ILLMProvider interface
- Singleton pattern prevents multiple initializations of API clients
- Allows runtime switching of providers without code changes [Source: architecture/tech-stack.md]

**File Structure** [Source: architecture/source-tree.md]:
```
src/infrastructure/external/llm/
├── __init__.py
├── provider_factory.py
└── providers/
    ├── __init__.py
    ├── base.py (ILLMProvider ABC)
    ├── openai_provider.py
    ├── anthropic_provider.py
    ├── ollama_provider.py
    └── openrouter_provider.py
```

### External API Details

**OpenAI** [Source: architecture/external-apis.md]:
- Base URL: https://api.openai.com
- Authentication: Bearer token (API key in header)
- Embeddings endpoint: POST `/v1/embeddings`
- Chat completions: POST `/v1/chat/completions`
- Rate limits: Varies by tier (handle 429 errors)
- Default models: gpt-4o-mini (completions), text-embedding-3-small (embeddings, 1536 dims)

**Anthropic** [Source: architecture/external-apis.md]:
- Base URL: https://api.anthropic.com
- Authentication: API key in `x-api-key` header
- Messages endpoint: POST `/v1/messages`
- Does NOT provide embeddings (use OpenAI or other provider)
- Default model: claude-3-haiku-20240307

**Ollama** [Source: architecture/external-apis.md]:
- Base URL: http://localhost:11434 (configurable)
- No authentication required (local service)
- Generate endpoint: POST `/api/generate` or `/api/chat`
- Embeddings endpoint: POST `/api/embeddings`
- Supports local models (llama2, mistral, etc.)
- Connection errors if Ollama not running

**OpenRouter** [Source: architecture/tech-stack.md]:
- Aggregates 100+ LLM models
- OpenAI-compatible API
- Authentication: Bearer token
- Does not provide embeddings directly

### Technical Implementation Notes

**Async HTTP Client**:
- Use httpx.AsyncClient for all API calls [Source: architecture/tech-stack.md]
- Configure timeouts (default: 30s for completions, 60s for embeddings)
- Enable retries for transient failures (e.g., network errors)
- Close clients properly in provider cleanup

**Error Handling** [Source: architecture/coding-standards.md#rule-4]:
- Define custom exceptions in `src/shared/utils/errors.py`:
  - LLMProviderError (base exception)
  - LLMAuthenticationError (401, 403)
  - LLMRateLimitError (429)
  - LLMConnectionError (network failures)
  - UnsupportedProviderError (unknown provider name)
- NEVER raise generic Exception or HTTPException from providers
- Log errors with provider name and error details

**Environment Variables** [Source: architecture/coding-standards.md#rule-5]:
- All API keys loaded from environment via settings.py
- NEVER hardcode API keys or secrets
- Validate required keys on app startup (fail fast if missing)

**Type Safety** [Source: architecture/coding-standards.md]:
- All methods MUST have full type hints
- Use `list[float]` for embeddings return type
- Use `AsyncIterator[str]` for streaming responses
- Pass MyPy checks

**Logging**:
- Log provider initialization with provider name
- Log API calls at DEBUG level (without sensitive data)
- Log errors at ERROR level with full context

### Settings Configuration Example

Add to `src/shared/config/settings.py`:
```python
class LLMSettings(BaseSettings):
    llm_provider: str = "openai"  # openai, anthropic, ollama, openrouter
    embedding_provider: str = "openai"
    
    openai_api_key: str | None = None
    anthropic_api_key: str | None = None
    ollama_base_url: str = "http://localhost:11434"
    openrouter_api_key: str | None = None
    
    default_llm_model: str = "gpt-4o-mini"
    default_embedding_model: str = "text-embedding-3-small"
    
    class Config:
        env_prefix = "LLM_"
```

Update `.env.example`:
```bash
# LLM Provider Configuration
LLM_PROVIDER=openai  # Options: openai, anthropic, ollama, openrouter
EMBEDDING_PROVIDER=openai

# OpenAI Configuration
LLM_OPENAI_API_KEY=sk-...
LLM_DEFAULT_LLM_MODEL=gpt-4o-mini
LLM_DEFAULT_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic Configuration (optional)
LLM_ANTHROPIC_API_KEY=sk-ant-...

# Ollama Configuration (optional, for local models)
LLM_OLLAMA_BASE_URL=http://localhost:11434

# OpenRouter Configuration (optional, for 100+ models)
LLM_OPENROUTER_API_KEY=sk-or-...
```

### Testing Standards

**Unit Tests** [Source: architecture/test-strategy-and-standards.md]:
- Mock all external API calls using pytest-httpx
- Test success cases: valid embeddings, completions, streaming
- Test error cases: rate limits (429), auth failures (401), network errors
- Test input validation and error handling
- Follow AAA pattern (Arrange, Act, Assert)
- Coverage goal: 90% for infrastructure layer

**Integration Tests** [Source: architecture/test-strategy-and-standards.md]:
- Stub external LLM APIs using httpx-responses or pytest-httpx
- Test ProviderFactory instantiation logic
- Test factory with different provider configurations
- Test FastAPI dependency injection integration
- Test singleton pattern (same instance returned)
- Verify ValueError raised for unsupported providers

**Test Isolation**:
- Each test should reset factory state
- Mock settings to avoid reading from actual environment
- No real API calls in tests (use mocks/stubs)

### Previous Story Insights

From Story 2.1 (Core Domain Models):
- ✅ Used dataclasses with slots for domain models - continue this pattern
- ✅ ABC interfaces for repositories worked well - apply same pattern to ILLMProvider
- ✅ Fresh pool pattern for test isolation - not needed here but consider for HTTP client lifecycle
- ✅ Type hints and validation crucial - all providers MUST have full type hints
- ✅ Custom exceptions better than generic ones - define LLM-specific exceptions
- Lesson: Start with interface definition, then implement concrete providers
- Lesson: Integration tests caught CASCADE delete issues - ensure factory integration tests cover dependency injection

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-07 | 0.1 | Initial draft created from Epic 2 requirements | Scrum Master (Bob) |
| 2025-11-07 | 0.2 | Implementation started - Core infrastructure complete | Dev Agent (James) |

## Dev Agent Record

### Agent Model Used
- Claude 3.7 Sonnet (New)

### Progress Summary

**Session 1 - 2025-11-07: Core Infrastructure Implementation**

**Completed Tasks:**
- ✅ Created ILLMProvider abstract interface with full type hints and docstrings
- ✅ Implemented OpenAIProvider with embed_text, generate_completion, and streaming
- ✅ Implemented AnthropicProvider (Note: no embeddings, system message handling)
- ✅ Implemented OllamaProvider for local models (no auth required)
- ✅ Implemented OpenRouterProvider for 100+ models
- ✅ Created ProviderFactory with singleton pattern
- ✅ Added LLMSettings to settings.py with all provider configurations
- ✅ Updated .env.example with comprehensive LLM configuration
- ✅ Added custom exceptions (LLMProviderError, LLMAuthenticationError, LLMRateLimitError, etc.)
- ✅ Created FastAPI dependencies (get_llm_provider, get_embedding_provider)
- ✅ Updated app lifespan to close providers on shutdown

**Files Created:**
1. `src/infrastructure/external/llm/providers/base.py` - ILLMProvider ABC (3 abstract methods)
2. `src/infrastructure/external/llm/providers/openai_provider.py` - OpenAI implementation
3. `src/infrastructure/external/llm/providers/anthropic_provider.py` - Anthropic implementation
4. `src/infrastructure/external/llm/providers/ollama_provider.py` - Ollama implementation
5. `src/infrastructure/external/llm/providers/openrouter_provider.py` - OpenRouter implementation
6. `src/infrastructure/external/llm/provider_factory.py` - Factory with singleton pattern
7. `tests/unit/infrastructure/external/llm/providers/` - Test directory structure

**Files Modified:**
1. `src/shared/config/settings.py` - Added LLMSettings dataclass
2. `src/shared/utils/errors.py` - Added 5 LLM-specific exceptions
3. `env_example` - Added comprehensive LLM provider configuration
4. `src/api/dependencies.py` - Added get_llm_provider and get_embedding_provider
5. `src/api/main.py` - Updated lifespan to close providers on shutdown

**Implementation Highlights:**
- All providers use httpx.AsyncClient for async HTTP requests
- Comprehensive error handling with custom exceptions (no generic exceptions)
- Proper timeout configuration (60s for embeddings, 30s for completions)
- Streaming support for all providers that support it
- Singleton pattern prevents duplicate API client initializations
- Resource cleanup via close() methods
- Full type hints and Google-style docstrings throughout

**Design Decisions:**
1. **Anthropic System Messages**: Extracted to separate `system` parameter (Anthropic API requirement)
2. **Ollama Connection Errors**: Special handling for "is Ollama running?" errors
3. **OpenRouter HTTP-Referer**: Added recommended header for OpenRouter API
4. **Lazy Initialization**: Providers initialized on first use via factory, not at app startup
5. **Singleton Cache**: Separate caches for LLM and embedding providers

**Lessons from Story 2.1 Applied:**
- ✅ ABC interface pattern (same as IRepository pattern)
- ✅ Full type hints on all methods and parameters
- ✅ Custom exceptions instead of generic ones
- ✅ Comprehensive docstrings with Args, Returns, Raises sections
- ✅ Clean separation of concerns (infrastructure layer only)

**Next Steps:**
- Create unit tests for all 4 providers (mocked HTTP calls)
- Create integration tests for ProviderFactory
- Test FastAPI dependency injection
- Run linting (Ruff, Black, MyPy)
- Complete remaining subtasks for full Story 2.2 completion

### Debug Log References
- No issues encountered during core implementation
- Code follows all patterns from Story 2.1
- Ready for testing phase

### Completion Notes
- Core implementation: 100% complete (all providers + factory + settings + dependencies)
- Testing: 0% complete (next phase)
- Documentation: 100% complete (comprehensive docstrings in all files)
