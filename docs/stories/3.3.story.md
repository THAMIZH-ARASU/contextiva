# Story 3.3: Advanced RAG - Agentic RAG (Synthesis)

## Status

**Ready for Review**

## Story

**As a** AI Agent Developer,
**I want** the RAG query endpoint to optionally return a synthesized, natural language answer,
**so that** my agent can consume a direct response instead of just raw chunks (FR10).

## Acceptance Criteria

1. The POST `/api/v1/rag/query` endpoint is updated to accept an optional boolean flag: `use_agentic_rag`.
2. If `use_agentic_rag` is true, the system takes the final (re-ranked) chunks, along with the original query, and passes them to an LLM provider (from Story 2.2) with a "summarize" or "answer based on context" prompt.
3. The API response is updated to include a new optional field, `synthesized_answer`, containing the LLM's natural language response.
4. Configuration in `settings.py` is added to enable/disable this feature by default (`RAG_USE_AGENTIC=false`).
5. E2E tests are updated to validate that a synthesized answer is correctly returned when the flag is enabled.

## Tasks / Subtasks

- [x] Update RAG Query Pydantic Schemas (AC: 1, 3)
  - [x] Update `RAGQueryRequest` in `src/api/v1/schemas/rag.py` to add optional field: `use_agentic_rag: bool = False` [Source: architecture/rest-api-spec.md, architecture/coding-standards.md#critical-rules]
  - [x] Update `RAGQueryResponse` schema to include optional field: `synthesized_answer: Optional[str] = None` [Source: architecture/coding-standards.md#critical-rules]
  - [x] Ensure all schemas use type hints and inherit from Pydantic BaseModel [Source: architecture/coding-standards.md#critical-rules]
  - [x] Add validation: synthesized_answer should only be present when use_agentic_rag=true

- [x] Add Agentic RAG Configuration to Settings (AC: 4)
  - [x] Update `src/shared/config/settings.py` RAGSettings class to add: `use_agentic_rag: bool = False` (default disabled) [Source: architecture/source-tree.md#shared-config]
  - [x] Add `agentic_rag_model: str = "stable-code:3b-code-q5_K_M"` (ollama model to use for synthesis)
  - [x] Add `agentic_rag_max_tokens: int = 1000` (max tokens for synthesized answer)
  - [x] Add `agentic_rag_temperature: float = 0.3` (temperature for synthesis, lower for factual answers)
  - [x] Add `agentic_rag_system_prompt: str` (default prompt for synthesis task)
  - [x] Ensure settings are loaded from environment variables with proper defaults [Source: architecture/coding-standards.md#critical-rules]

- [x] Create Synthesis Service in Application Layer (AC: 2)
  - [x] Create new file `src/application/services/synthesis_service.py` [Source: architecture/source-tree.md#application-layer]
  - [x] Create `SynthesisService` class with async `synthesize()` method
  - [x] Method signature: `async synthesize(query: str, chunks: List[KnowledgeItem], settings: RAGSettings) -> str`
  - [x] Build synthesis prompt: Include system prompt from settings, user query, and all chunk texts as context
  - [x] Call LLM provider via `ILLMProvider.generate()` method with configured model, max_tokens, and temperature [Source: architecture/external-apis.md]
  - [x] Extract synthesized answer from LLM response
  - [x] Handle errors gracefully: If LLM fails, log warning and return None (graceful degradation)
  - [x] All methods MUST be async [Source: architecture/coding-standards.md#language-specific-guidelines]
  - [x] Add comprehensive docstrings and type hints [Source: architecture/coding-standards.md#language-specific-guidelines]

- [x] Integrate Synthesis Service into QueryKnowledgeUseCase (AC: 2)
  - [x] Update `src/application/use_cases/knowledge/query_knowledge.py` [Source: architecture/source-tree.md#application-layer]
  - [x] Inject `SynthesisService` as dependency in `QueryKnowledgeUseCase.__init__()`
  - [x] Add `use_agentic_rag` parameter to `execute()` method signature
  - [x] After re-ranking step (if enabled) or hybrid search (if enabled) or vector search (baseline):
    - [x] If `use_agentic_rag` is true, call `SynthesisService.synthesize(query_text, final_chunks, settings)`
    - [x] Store synthesized answer in `QueryKnowledgeResult.synthesized_answer` field
  - [x] Update `QueryKnowledgeResult` class to include `synthesized_answer: Optional[str] = None` attribute
  - [x] Ensure synthesized_answer is None when use_agentic_rag=false
  - [x] Handle errors: If synthesis fails, log error and set synthesized_answer=None (do not fail the entire query)
  - [x] Maintain async/await pattern [Source: architecture/coding-standards.md#critical-rules]

- [x] Update RAG API Endpoint (AC: 1, 3)
  - [x] Update `src/api/v1/routes/rag.py` endpoint `/api/v1/rag/query` [Source: architecture/source-tree.md#api-layer]
  - [x] Pass `request.use_agentic_rag` to `QueryKnowledgeUseCase.execute()`
  - [x] Map `result.synthesized_answer` to `RAGQueryResponse.synthesized_answer`
  - [x] NO business logic in this layer - only validation and orchestration [Source: architecture/coding-standards.md#critical-rules]
  - [x] Ensure response includes synthesized_answer when flag is enabled

- [x] Create Unit Tests for SynthesisService (AC: 2)
  - [x] Create `tests/unit/application/services/test_synthesis_service.py` [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [x] Test successful synthesis with valid inputs
  - [x] Test synthesis with empty chunks list (should return appropriate message)
  - [x] Test synthesis with single chunk vs multiple chunks
  - [x] Test LLM provider failure handling (graceful degradation)
  - [x] Test prompt construction includes query and all chunks
  - [x] Test max_tokens and temperature settings are passed to LLM
  - [x] Mock `ILLMProvider` interface [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [x] Follow AAA pattern (Arrange, Act, Assert) [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [x] Target 90%+ coverage for service [Source: architecture/test-strategy-and-standards.md#coverage-goals]

- [x] Update Unit Tests for QueryKnowledgeUseCase (AC: 2)
  - [x] Update `tests/unit/application/use_cases/knowledge/test_query_knowledge.py` [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [x] Add test: successful query with use_agentic_rag=true returns synthesized_answer
  - [x] Add test: query with use_agentic_rag=false does NOT include synthesized_answer
  - [x] Add test: synthesis service failure does not fail the query (graceful degradation)
  - [x] Add test: synthesized_answer is None when synthesis is disabled
  - [x] Mock `SynthesisService` dependency
  - [x] Verify synthesized_answer is correctly populated in result
  - [x] Target 90%+ coverage [Source: architecture/test-strategy-and-standards.md#coverage-goals]

- [x] Create E2E Tests for Agentic RAG (AC: 5)
  - [x] Update `tests/e2e/api/v1/test_rag.py` [Source: architecture/test-strategy-and-standards.md#end-to-end-e2e-tests]
  - [x] Add test: POST /api/v1/rag/query with use_agentic_rag=true returns synthesized_answer
  - [x] Add test: synthesized_answer is string and not empty when flag is enabled
  - [x] Add test: synthesized_answer is None when use_agentic_rag=false
  - [x] Add test: combined mode (use_hybrid_search, use_re_ranking, use_agentic_rag all true)
  - [x] Add test: synthesized_answer content is relevant to query (basic validation)
  - [x] Mock LLM provider for synthesis (use `unittest.mock.patch`) to avoid external dependencies
  - [x] Use httpx as test client with proper auth headers [Source: architecture/test-strategy-and-standards.md#end-to-end-e2e-tests]
  - [x] Tests create own data and clean up [Source: architecture/test-strategy-and-standards.md#test-data-management]

## Dev Notes

### Previous Story Insights

From Story 3.2 (Advanced RAG - Hybrid Search & Re-ranking):
- The QueryKnowledgeUseCase now supports optional hybrid_search and re_ranking flags
- LLM providers are accessed via ILLMProvider interface from the LLM Provider Factory (Story 2.2)
- Re-ranking service successfully uses LLM provider for relevance scoring with robust error handling
- All advanced features use graceful degradation: if a feature fails, the query still returns results
- Redis caching is implemented for query results with configurable TTL
- The use case follows a sequential pipeline: vector search → (optional) hybrid search → (optional) re-ranking
- Response schemas include optional fields (bm25_score, rerank_score) that are only populated when features are enabled

From Story 3.1 (RAG Retrieval API - Core Query):
- POST `/api/v1/rag/query` endpoint exists and is secured with JWT authentication
- RAGQueryRequest and RAGQueryResponse schemas are in `src/api/v1/schemas/rag.py`
- QueryKnowledgeUseCase orchestrates the RAG query workflow in `src/application/use_cases/knowledge/query_knowledge.py`
- EmbeddingService from Story 2.2 is used via ProviderFactory for generating query embeddings
- KnowledgeRepository performs vector similarity search with pgvector

### Data Models

**KnowledgeItem Entity** [Source: architecture/data-models.md#knowledgeitem]:
- `id`: UUID - Primary key
- `document_id`: UUID - Foreign key to Document
- `chunk_text`: str - The raw text of the chunk (this will be passed to LLM for synthesis)
- `embedding`: Vector - The pgvector embedding
- `metadata`: JSON - Additional context (page number, source URL, chunk indices)
- Relationship: Belongs to ONE Document

**Synthesis Flow**:
1. After retrieval and optional re-ranking, we have a list of top-K KnowledgeItems
2. Extract chunk_text from each KnowledgeItem
3. Build synthesis prompt: system prompt + user query + concatenated chunks as context
4. Call LLM provider to generate natural language answer
5. Return synthesized answer in response

### API Specifications

**Endpoint**: POST `/api/v1/rag/query` [Source: architecture/rest-api-spec.md]:
- Authentication: JWT Bearer token (from Story 1.4)
- Request Body: RAGQueryRequest (project_id, query_text, optional top_k, use_hybrid_search, use_re_ranking, **use_agentic_rag**)
- Response: 200 OK with RAGQueryResponse (results array, query_id, total_results, **synthesized_answer**)
- Error Responses: 401 (Unauthorized), 403 (Forbidden), 404 (Not Found), 422 (Validation Error)

**Schema Updates**:
- Add `use_agentic_rag: bool = False` to RAGQueryRequest
- Add `synthesized_answer: Optional[str] = None` to RAGQueryResponse

### Component Specifications

**Application Layer - SynthesisService** [Source: architecture/source-tree.md#application-layer]:
- NEW SERVICE in `src/application/services/synthesis_service.py`
- Purpose: Generate natural language answers from retrieved knowledge chunks
- Dependencies: ILLMProvider (from LLM Provider Factory)
- Method: `async synthesize(query: str, chunks: List[KnowledgeItem], settings: RAGSettings) -> Optional[str]`
- Error Handling: Return None if LLM fails (graceful degradation)

**Application Layer - QueryKnowledgeUseCase** [Source: architecture/source-tree.md#application-layer]:
- Update existing use case in `src/application/use_cases/knowledge/query_knowledge.py`
- Add SynthesisService as dependency
- Add use_agentic_rag parameter to execute() method
- Call synthesis service after retrieval/re-ranking if flag is enabled
- Update QueryKnowledgeResult to include synthesized_answer field

**Infrastructure Layer - LLM Provider** [Source: architecture/external-apis.md]:
- Use existing ILLMProvider interface from Story 2.2
- Call `generate()` method for synthesis (same as re-ranking in Story 3.2)
- Primary provider: OpenAI (gpt-4o-mini)
- Alternate providers: Anthropic (claude-3-haiku), Ollama (local), OpenRouter

### File Locations

Based on Clean Architecture and Source Tree [Source: architecture/source-tree.md]:
- Request/Response Schemas: `src/api/v1/schemas/rag.py` (UPDATE EXISTING)
- API Endpoint: `src/api/v1/routes/rag.py` (UPDATE EXISTING)
- Synthesis Service: `src/application/services/synthesis_service.py` (NEW FILE)
- Use Case: `src/application/use_cases/knowledge/query_knowledge.py` (UPDATE EXISTING)
- Settings: `src/shared/config/settings.py` (UPDATE EXISTING)
- Unit Tests - Service: `tests/unit/application/services/test_synthesis_service.py` (NEW FILE)
- Unit Tests - Use Case: `tests/unit/application/use_cases/knowledge/test_query_knowledge.py` (UPDATE EXISTING)
- E2E Tests: `tests/e2e/api/v1/test_rag.py` (UPDATE EXISTING)

### Technical Constraints

**LLM Provider Integration** [Source: architecture/external-apis.md]:
- Base URL: https://api.openai.com (for OpenAI)
- Authentication: API Key (Bearer Token) from environment variable `LLM_API_KEY`
- Endpoint: POST `/v1/chat/completions` (for synthesis)
- Rate Limits: Varies by API key tier
- Error Handling: MUST handle rate limits, timeouts, and API errors gracefully

**Prompt Engineering** [Source: architecture/coding-standards.md]:
- System Prompt: Should instruct LLM to answer based ONLY on provided context
- User Prompt: Include original query and all chunk texts
- Temperature: 0.3 (lower for factual answers, avoid hallucinations)
- Max Tokens: 1000 (configurable, sufficient for most answers)

**Performance Considerations**:
- Synthesis adds LLM API latency (typically 1-3 seconds)
- Should be optional (default disabled) to avoid unnecessary cost/latency
- Cache the entire RAGQueryResponse including synthesized_answer (Redis from Story 3.2)

**Security** [Source: architecture/coding-standards.md#critical-rules]:
- NEVER include sensitive data in LLM prompts
- Validate that user has access to project before synthesis
- LLM responses should not be trusted for security decisions

### Testing

**Test Strategy** [Source: architecture/test-strategy-and-standards.md]:

**Unit Tests** (Target: 90%+ coverage):
- SynthesisService: 6+ tests covering success, empty chunks, errors, prompt construction
- QueryKnowledgeUseCase: 4+ new tests for agentic_rag flag handling

**Integration Tests** (Optional for this story):
- No new integration tests required (LLM provider interface already tested in Story 2.2)
- Can validate with real LLM provider in manual testing

**E2E Tests** (5+ tests):
- Test synthesized_answer returned when use_agentic_rag=true
- Test synthesized_answer is None when use_agentic_rag=false
- Test combined mode (all flags enabled)
- Test response schema validation
- Test graceful degradation if synthesis fails

**Mocking Strategy** [Source: architecture/test-strategy-and-standards.md#integration-tests]:
- Mock ILLMProvider for unit tests (return fake synthesized answer)
- Mock LLM provider in E2E tests using `unittest.mock.patch`
- Avoid external LLM API calls in automated tests

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-12 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

Claude 3.5 Sonnet (GitHub Copilot)

### Debug Log References

None - Implementation completed without blocking issues

### Completion Notes List

- ✅ All tasks and subtasks completed successfully
- ✅ Added `use_agentic_rag` flag to RAGQueryRequest schema
- ✅ Added `synthesized_answer` field to RAGQueryResponse schema
- ✅ Created comprehensive RAGSettings configuration for agentic RAG
- ✅ Implemented SynthesisService with proper error handling and graceful degradation
- ✅ Integrated synthesis service into QueryKnowledgeUseCase
- ✅ Updated Redis cache to include use_agentic parameter in cache key generation
- ✅ Updated RAG API endpoint to pass through use_agentic_rag flag
- ✅ Created 7 unit tests for SynthesisService (100% pass rate)
- ✅ Created 4 unit tests for QueryKnowledgeUseCase agentic RAG functionality (100% pass rate)
- ✅ Created 6 E2E tests for agentic RAG endpoint (ready for execution)
- ✅ All unit tests passing: 11/11 (7 SynthesisService + 4 QueryKnowledgeUseCase)
- ✅ Graceful degradation implemented: synthesis failure does not break RAG query
- ✅ Clean Architecture maintained: No circular dependencies, proper layer separation
- ✅ Ollama configured as default LLM provider for local development

### File List

**Created:**
- `src/application/services/synthesis_service.py` - Service for synthesizing natural language answers
- `tests/unit/application/services/test_synthesis_service.py` - Unit tests for SynthesisService (7 tests)

**Modified:**
- `src/api/v1/schemas/rag.py` - Added use_agentic_rag and synthesized_answer fields
- `src/shared/config/settings.py` - Added agentic RAG configuration to RAGSettings
- `src/application/use_cases/knowledge/query_knowledge.py` - Integrated synthesis service
- `src/infrastructure/cache/redis_cache.py` - Updated cache key generation to include use_agentic flag
- `src/api/v1/routes/rag.py` - Updated endpoint to pass use_agentic_rag and return synthesized_answer
- `tests/unit/application/use_cases/knowledge/test_query_knowledge.py` - Added 4 agentic RAG tests
- `tests/e2e/api/v1/test_rag.py` - Added 6 E2E tests for agentic RAG

## QA Results

(To be populated by QA Agent)
