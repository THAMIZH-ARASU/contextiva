# Story 3.3: Advanced RAG - Agentic RAG (Synthesis)

## Status

**Draft**

## Story

**As a** AI Agent Developer,
**I want** the RAG query endpoint to optionally return a synthesized, natural language answer,
**so that** my agent can consume a direct response instead of just raw chunks (FR10).

## Acceptance Criteria

1. The POST `/api/v1/rag/query` endpoint is updated to accept an optional boolean flag: `use_agentic_rag`.
2. If `use_agentic_rag` is true, the system takes the final (re-ranked) chunks, along with the original query, and passes them to an LLM provider (from Story 2.2) with a "summarize" or "answer based on context" prompt.
3. The API response is updated to include a new optional field, `synthesized_answer`, containing the LLM's natural language response.
4. Configuration in `settings.py` is added to enable/disable this feature by default (`RAG_USE_AGENTIC=false`).
5. E2E tests are updated to validate that a synthesized answer is correctly returned when the flag is enabled.

## Tasks / Subtasks

- [ ] Update RAG Query Pydantic Schemas (AC: 1, 3)
  - [ ] Update `RAGQueryRequest` in `src/api/v1/schemas/rag.py` to add optional field: `use_agentic_rag: bool = False` [Source: architecture/rest-api-spec.md, architecture/coding-standards.md#critical-rules]
  - [ ] Update `RAGQueryResponse` schema to include optional field: `synthesized_answer: Optional[str] = None` [Source: architecture/coding-standards.md#critical-rules]
  - [ ] Ensure all schemas use type hints and inherit from Pydantic BaseModel [Source: architecture/coding-standards.md#critical-rules]
  - [ ] Add validation: synthesized_answer should only be present when use_agentic_rag=true

- [ ] Add Agentic RAG Configuration to Settings (AC: 4)
  - [ ] Update `src/shared/config/settings.py` RAGSettings class to add: `use_agentic_rag: bool = False` (default disabled) [Source: architecture/source-tree.md#shared-config]
  - [ ] Add `agentic_rag_model: str = "gpt-4o-mini"` (model to use for synthesis)
  - [ ] Add `agentic_rag_max_tokens: int = 1000` (max tokens for synthesized answer)
  - [ ] Add `agentic_rag_temperature: float = 0.3` (temperature for synthesis, lower for factual answers)
  - [ ] Add `agentic_rag_system_prompt: str` (default prompt for synthesis task)
  - [ ] Ensure settings are loaded from environment variables with proper defaults [Source: architecture/coding-standards.md#critical-rules]

- [ ] Create Synthesis Service in Application Layer (AC: 2)
  - [ ] Create new file `src/application/services/synthesis_service.py` [Source: architecture/source-tree.md#application-layer]
  - [ ] Create `SynthesisService` class with async `synthesize()` method
  - [ ] Method signature: `async synthesize(query: str, chunks: List[KnowledgeItem], settings: RAGSettings) -> str`
  - [ ] Build synthesis prompt: Include system prompt from settings, user query, and all chunk texts as context
  - [ ] Call LLM provider via `ILLMProvider.generate()` method with configured model, max_tokens, and temperature [Source: architecture/external-apis.md]
  - [ ] Extract synthesized answer from LLM response
  - [ ] Handle errors gracefully: If LLM fails, log warning and return None (graceful degradation)
  - [ ] All methods MUST be async [Source: architecture/coding-standards.md#language-specific-guidelines]
  - [ ] Add comprehensive docstrings and type hints [Source: architecture/coding-standards.md#language-specific-guidelines]

- [ ] Integrate Synthesis Service into QueryKnowledgeUseCase (AC: 2)
  - [ ] Update `src/application/use_cases/knowledge/query_knowledge.py` [Source: architecture/source-tree.md#application-layer]
  - [ ] Inject `SynthesisService` as dependency in `QueryKnowledgeUseCase.__init__()`
  - [ ] Add `use_agentic_rag` parameter to `execute()` method signature
  - [ ] After re-ranking step (if enabled) or hybrid search (if enabled) or vector search (baseline):
    - [ ] If `use_agentic_rag` is true, call `SynthesisService.synthesize(query_text, final_chunks, settings)`
    - [ ] Store synthesized answer in `QueryKnowledgeResult.synthesized_answer` field
  - [ ] Update `QueryKnowledgeResult` class to include `synthesized_answer: Optional[str] = None` attribute
  - [ ] Ensure synthesized_answer is None when use_agentic_rag=false
  - [ ] Handle errors: If synthesis fails, log error and set synthesized_answer=None (do not fail the entire query)
  - [ ] Maintain async/await pattern [Source: architecture/coding-standards.md#critical-rules]

- [ ] Update RAG API Endpoint (AC: 1, 3)
  - [ ] Update `src/api/v1/routes/rag.py` endpoint `/api/v1/rag/query` [Source: architecture/source-tree.md#api-layer]
  - [ ] Pass `request.use_agentic_rag` to `QueryKnowledgeUseCase.execute()`
  - [ ] Map `result.synthesized_answer` to `RAGQueryResponse.synthesized_answer`
  - [ ] NO business logic in this layer - only validation and orchestration [Source: architecture/coding-standards.md#critical-rules]
  - [ ] Ensure response includes synthesized_answer when flag is enabled

- [ ] Create Unit Tests for SynthesisService (AC: 2)
  - [ ] Create `tests/unit/application/services/test_synthesis_service.py` [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [ ] Test successful synthesis with valid inputs
  - [ ] Test synthesis with empty chunks list (should return appropriate message)
  - [ ] Test synthesis with single chunk vs multiple chunks
  - [ ] Test LLM provider failure handling (graceful degradation)
  - [ ] Test prompt construction includes query and all chunks
  - [ ] Test max_tokens and temperature settings are passed to LLM
  - [ ] Mock `ILLMProvider` interface [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [ ] Follow AAA pattern (Arrange, Act, Assert) [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [ ] Target 90%+ coverage for service [Source: architecture/test-strategy-and-standards.md#coverage-goals]

- [ ] Update Unit Tests for QueryKnowledgeUseCase (AC: 2)
  - [ ] Update `tests/unit/application/use_cases/knowledge/test_query_knowledge.py` [Source: architecture/test-strategy-and-standards.md#unit-tests]
  - [ ] Add test: successful query with use_agentic_rag=true returns synthesized_answer
  - [ ] Add test: query with use_agentic_rag=false does NOT include synthesized_answer
  - [ ] Add test: synthesis service failure does not fail the query (graceful degradation)
  - [ ] Add test: synthesized_answer is None when synthesis is disabled
  - [ ] Mock `SynthesisService` dependency
  - [ ] Verify synthesized_answer is correctly populated in result
  - [ ] Target 90%+ coverage [Source: architecture/test-strategy-and-standards.md#coverage-goals]

- [ ] Create E2E Tests for Agentic RAG (AC: 5)
  - [ ] Update `tests/e2e/api/v1/test_rag.py` [Source: architecture/test-strategy-and-standards.md#end-to-end-e2e-tests]
  - [ ] Add test: POST /api/v1/rag/query with use_agentic_rag=true returns synthesized_answer
  - [ ] Add test: synthesized_answer is string and not empty when flag is enabled
  - [ ] Add test: synthesized_answer is None when use_agentic_rag=false
  - [ ] Add test: combined mode (use_hybrid_search, use_re_ranking, use_agentic_rag all true)
  - [ ] Add test: synthesized_answer content is relevant to query (basic validation)
  - [ ] Mock LLM provider for synthesis (use `unittest.mock.patch`) to avoid external dependencies
  - [ ] Use httpx as test client with proper auth headers [Source: architecture/test-strategy-and-standards.md#end-to-end-e2e-tests]
  - [ ] Tests create own data and clean up [Source: architecture/test-strategy-and-standards.md#test-data-management]

## Dev Notes

### Previous Story Insights

From Story 3.2 (Advanced RAG - Hybrid Search & Re-ranking):
- The QueryKnowledgeUseCase now supports optional hybrid_search and re_ranking flags
- LLM providers are accessed via ILLMProvider interface from the LLM Provider Factory (Story 2.2)
- Re-ranking service successfully uses LLM provider for relevance scoring with robust error handling
- All advanced features use graceful degradation: if a feature fails, the query still returns results
- Redis caching is implemented for query results with configurable TTL
- The use case follows a sequential pipeline: vector search → (optional) hybrid search → (optional) re-ranking
- Response schemas include optional fields (bm25_score, rerank_score) that are only populated when features are enabled

From Story 3.1 (RAG Retrieval API - Core Query):
- POST `/api/v1/rag/query` endpoint exists and is secured with JWT authentication
- RAGQueryRequest and RAGQueryResponse schemas are in `src/api/v1/schemas/rag.py`
- QueryKnowledgeUseCase orchestrates the RAG query workflow in `src/application/use_cases/knowledge/query_knowledge.py`
- EmbeddingService from Story 2.2 is used via ProviderFactory for generating query embeddings
- KnowledgeRepository performs vector similarity search with pgvector

### Data Models

**KnowledgeItem Entity** [Source: architecture/data-models.md#knowledgeitem]:
- `id`: UUID - Primary key
- `document_id`: UUID - Foreign key to Document
- `chunk_text`: str - The raw text of the chunk (this will be passed to LLM for synthesis)
- `embedding`: Vector - The pgvector embedding
- `metadata`: JSON - Additional context (page number, source URL, chunk indices)
- Relationship: Belongs to ONE Document

**Synthesis Flow**:
1. After retrieval and optional re-ranking, we have a list of top-K KnowledgeItems
2. Extract chunk_text from each KnowledgeItem
3. Build synthesis prompt: system prompt + user query + concatenated chunks as context
4. Call LLM provider to generate natural language answer
5. Return synthesized answer in response

### API Specifications

**Endpoint**: POST `/api/v1/rag/query` [Source: architecture/rest-api-spec.md]:
- Authentication: JWT Bearer token (from Story 1.4)
- Request Body: RAGQueryRequest (project_id, query_text, optional top_k, use_hybrid_search, use_re_ranking, **use_agentic_rag**)
- Response: 200 OK with RAGQueryResponse (results array, query_id, total_results, **synthesized_answer**)
- Error Responses: 401 (Unauthorized), 403 (Forbidden), 404 (Not Found), 422 (Validation Error)

**Schema Updates**:
- Add `use_agentic_rag: bool = False` to RAGQueryRequest
- Add `synthesized_answer: Optional[str] = None` to RAGQueryResponse

### Component Specifications

**Application Layer - SynthesisService** [Source: architecture/source-tree.md#application-layer]:
- NEW SERVICE in `src/application/services/synthesis_service.py`
- Purpose: Generate natural language answers from retrieved knowledge chunks
- Dependencies: ILLMProvider (from LLM Provider Factory)
- Method: `async synthesize(query: str, chunks: List[KnowledgeItem], settings: RAGSettings) -> Optional[str]`
- Error Handling: Return None if LLM fails (graceful degradation)

**Application Layer - QueryKnowledgeUseCase** [Source: architecture/source-tree.md#application-layer]:
- Update existing use case in `src/application/use_cases/knowledge/query_knowledge.py`
- Add SynthesisService as dependency
- Add use_agentic_rag parameter to execute() method
- Call synthesis service after retrieval/re-ranking if flag is enabled
- Update QueryKnowledgeResult to include synthesized_answer field

**Infrastructure Layer - LLM Provider** [Source: architecture/external-apis.md]:
- Use existing ILLMProvider interface from Story 2.2
- Call `generate()` method for synthesis (same as re-ranking in Story 3.2)
- Primary provider: OpenAI (gpt-4o-mini)
- Alternate providers: Anthropic (claude-3-haiku), Ollama (local), OpenRouter

### File Locations

Based on Clean Architecture and Source Tree [Source: architecture/source-tree.md]:
- Request/Response Schemas: `src/api/v1/schemas/rag.py` (UPDATE EXISTING)
- API Endpoint: `src/api/v1/routes/rag.py` (UPDATE EXISTING)
- Synthesis Service: `src/application/services/synthesis_service.py` (NEW FILE)
- Use Case: `src/application/use_cases/knowledge/query_knowledge.py` (UPDATE EXISTING)
- Settings: `src/shared/config/settings.py` (UPDATE EXISTING)
- Unit Tests - Service: `tests/unit/application/services/test_synthesis_service.py` (NEW FILE)
- Unit Tests - Use Case: `tests/unit/application/use_cases/knowledge/test_query_knowledge.py` (UPDATE EXISTING)
- E2E Tests: `tests/e2e/api/v1/test_rag.py` (UPDATE EXISTING)

### Technical Constraints

**LLM Provider Integration** [Source: architecture/external-apis.md]:
- Base URL: https://api.openai.com (for OpenAI)
- Authentication: API Key (Bearer Token) from environment variable `LLM_API_KEY`
- Endpoint: POST `/v1/chat/completions` (for synthesis)
- Rate Limits: Varies by API key tier
- Error Handling: MUST handle rate limits, timeouts, and API errors gracefully

**Prompt Engineering** [Source: architecture/coding-standards.md]:
- System Prompt: Should instruct LLM to answer based ONLY on provided context
- User Prompt: Include original query and all chunk texts
- Temperature: 0.3 (lower for factual answers, avoid hallucinations)
- Max Tokens: 1000 (configurable, sufficient for most answers)

**Performance Considerations**:
- Synthesis adds LLM API latency (typically 1-3 seconds)
- Should be optional (default disabled) to avoid unnecessary cost/latency
- Cache the entire RAGQueryResponse including synthesized_answer (Redis from Story 3.2)

**Security** [Source: architecture/coding-standards.md#critical-rules]:
- NEVER include sensitive data in LLM prompts
- Validate that user has access to project before synthesis
- LLM responses should not be trusted for security decisions

### Testing

**Test Strategy** [Source: architecture/test-strategy-and-standards.md]:

**Unit Tests** (Target: 90%+ coverage):
- SynthesisService: 6+ tests covering success, empty chunks, errors, prompt construction
- QueryKnowledgeUseCase: 4+ new tests for agentic_rag flag handling

**Integration Tests** (Optional for this story):
- No new integration tests required (LLM provider interface already tested in Story 2.2)
- Can validate with real LLM provider in manual testing

**E2E Tests** (5+ tests):
- Test synthesized_answer returned when use_agentic_rag=true
- Test synthesized_answer is None when use_agentic_rag=false
- Test combined mode (all flags enabled)
- Test response schema validation
- Test graceful degradation if synthesis fails

**Mocking Strategy** [Source: architecture/test-strategy-and-standards.md#integration-tests]:
- Mock ILLMProvider for unit tests (return fake synthesized answer)
- Mock LLM provider in E2E tests using `unittest.mock.patch`
- Avoid external LLM API calls in automated tests

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-12 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

(To be populated by Dev Agent)

### Debug Log References

(To be populated by Dev Agent)

### Completion Notes List

(To be populated by Dev Agent)

### File List

(To be populated by Dev Agent)

## QA Results

(To be populated by QA Agent)
