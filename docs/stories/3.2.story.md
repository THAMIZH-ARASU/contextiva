# Story 3.2: Advanced RAG - Hybrid Search, Re-ranking & Redis Caching

## Status

**Draft**

## Story

**As a** AI Agent Developer,
**I want** the RAG query endpoint to optionally support Hybrid Search, Re-ranking, and Redis caching,
**so that** I can improve the relevance and performance of my search results (FR10, NFR6).

## Acceptance Criteria

1. The POST `/api/v1/rag/query` endpoint is updated to accept optional boolean flags: `use_hybrid_search` and `use_re_ranking`.
2. If `use_hybrid_search` is true, the system performs both vector search (from 3.1) and traditional keyword search (e.g., BM25/full-text search) and merges the results.
3. If `use_re_ranking` is true, the initial set of retrieved chunks (from AC 3.1.5 or 3.2.2) is passed to an LLM provider (from Story 2.2) to re-rank them for relevance to the original query.
4. Configuration in `settings.py` is added to enable/disable these features by default (e.g., RAG_USE_HYBRID_SEARCH=false, RAG_USE_RERANKING=false).
5. The system MUST use Redis to cache RAG query results (NFR6), with configurable TTL.
6. Cache keys MUST be based on project_id, query_text, and search parameters (use_hybrid_search, use_re_ranking, top_k).
7. E2E tests are updated to validate the behavior when these flags are enabled and disabled, including cache hit/miss scenarios.

## Tasks / Subtasks

- [ ] Update RAG Query Pydantic Schemas (AC: 1)
  - [ ] Update `RAGQueryRequest` in `src/api/v1/schemas/rag.py` to add optional fields: `use_hybrid_search: bool = False` and `use_re_ranking: bool = False` [Source: architecture/rest-api-spec.md, architecture/coding-standards.md]
  - [ ] Update `KnowledgeItemResult` schema to include optional `bm25_score: Optional[float] = None` for keyword search results [Source: architecture/coding-standards.md#critical-rules]
  - [ ] Update `KnowledgeItemResult` schema to include optional `rerank_score: Optional[float] = None` for re-ranking results [Source: architecture/coding-standards.md#critical-rules]
  - [ ] Ensure all schemas use type hints and inherit from Pydantic BaseModel [Source: architecture/coding-standards.md#critical-rules]

- [ ] Add Advanced RAG Configuration to Settings (AC: 4, 5, 6)
  - [ ] Update `src/shared/config/settings.py` RAGSettings class to add: `use_hybrid_search: bool = False` (default disabled) [Source: architecture/source-tree.md#shared-config]
  - [ ] Add `use_reranking: bool = False` (default disabled) to RAGSettings
  - [ ] Add `hybrid_search_weight_vector: float = 0.7` (weight for vector search in hybrid merge)
  - [ ] Add `hybrid_search_weight_bm25: float = 0.3` (weight for BM25 search in hybrid merge)
  - [ ] Add `reranking_model: str = "gpt-4o-mini"` (model to use for re-ranking)
  - [ ] Add `reranking_top_k: int = 10` (number of results to send to re-ranker)
  - [ ] Add `cache_enabled: bool = True` (enable/disable Redis caching - NFR6)
  - [ ] Add `cache_ttl: int = 3600` (cache TTL in seconds, default 1 hour)
  - [ ] Add `cache_key_prefix: str = "rag:query:"` (Redis key prefix for RAG queries)
  - [ ] Ensure settings are loaded from environment variables with proper defaults [Source: architecture/coding-standards.md#critical-rules]

- [ ] Implement PostgreSQL Full-Text Search (BM25) in KnowledgeRepository (AC: 2)
  - [ ] Add async method `keyword_search(project_id: UUID, query_text: str, top_k: int) -> List[Tuple[KnowledgeItem, float]]` to IKnowledgeRepository interface in `src/domain/models/knowledge.py` [Source: architecture/coding-standards.md#critical-rules]
  - [ ] Implement the method in KnowledgeRepository (`src/infrastructure/database/repositories/knowledge_repository.py`)
  - [ ] Use PostgreSQL's `to_tsvector()` and `to_tsquery()` for full-text search [Source: architecture/database-schema.md]
  - [ ] Use `ts_rank_cd()` function to compute BM25-like relevance scores
  - [ ] Query MUST filter by project_id via JOIN with documents table
  - [ ] Return KnowledgeItems with BM25 scores, ordered by relevance (highest first), limited to top_k
  - [ ] Handle edge case: return empty list if no matching documents exist for project
  - [ ] All database operations MUST use parameterized queries [Source: architecture/coding-standards.md#critical-rules]

- [ ] Create Migration for Full-Text Search Index (AC: 2)
  - [ ] Create new Alembic migration in `migration/versions/` to add GIN index on `chunk_text` column of `knowledge_items` table [Source: architecture/database-schema.md]
  - [ ] Migration SQL: `CREATE INDEX IF NOT EXISTS idx_knowledge_items_fulltext ON knowledge_items USING gin(to_tsvector('english', chunk_text));`
  - [ ] Run migration: `alembic upgrade head`

- [ ] Implement Hybrid Search Merge Logic (AC: 2)
  - [ ] Create new service class `HybridSearchService` in `src/application/services/hybrid_search_service.py` [Source: architecture/source-tree.md#application-services]
  - [ ] Implement `async def merge_results(vector_results: List[Tuple[KnowledgeItem, float]], keyword_results: List[Tuple[KnowledgeItem, float]], weight_vector: float, weight_bm25: float) -> List[Tuple[KnowledgeItem, float]]` method
  - [ ] Use Reciprocal Rank Fusion (RRF) algorithm: `score = weight_vector * (1 / (rank_vector + 60)) + weight_bm25 * (1 / (rank_bm25 + 60))`
  - [ ] Deduplicate results by KnowledgeItem.id, keeping highest combined score
  - [ ] Return merged list sorted by combined score (descending)
  - [ ] Follow Clean Architecture: service MUST NOT import from api or infrastructure layers [Source: architecture/coding-standards.md#critical-rules]

- [ ] Implement Re-ranking with LLM Provider (AC: 3)
  - [ ] Create new service class `RerankingService` in `src/application/services/reranking_service.py` [Source: architecture/source-tree.md#application-services]
  - [ ] Implement `async def rerank_chunks(query_text: str, chunks: List[KnowledgeItem], llm_provider: ILLMProvider, top_k: int) -> List[Tuple[KnowledgeItem, float]]` method
  - [ ] Construct re-ranking prompt: "Given the query: '{query}', rank the following text chunks by relevance (most relevant first). Return only the ranking as JSON array of indices.\n\nChunks:\n0: {chunk_0}\n1: {chunk_1}\n..."
  - [ ] Call `llm_provider.generate_completion(prompt)` to get re-ranked indices [Source: Story 2.2 - LLM Provider Factory]
  - [ ] Parse LLM response as JSON array of integers (chunk indices)
  - [ ] Return KnowledgeItems in re-ranked order with normalized scores (1.0 for top, decreasing)
  - [ ] Handle errors: if LLM fails or returns invalid JSON, return original order with warning log
  - [ ] Follow Clean Architecture: service MUST NOT import from api or infrastructure layers [Source: architecture/coding-standards.md#critical-rules]

- [ ] Update QueryKnowledgeUseCase for Advanced RAG (AC: 2, 3)
  - [ ] Update `QueryKnowledgeUseCase` in `src/application/use_cases/knowledge/query_knowledge.py` to accept new optional parameters: `use_hybrid_search: bool`, `use_re_ranking: bool`
  - [ ] If `use_hybrid_search` is False, use existing vector search logic (from Story 3.1)
  - [ ] If `use_hybrid_search` is True:
    - [ ] Perform vector search (existing logic)
    - [ ] Perform keyword search via `knowledge_repo.keyword_search()`
    - [ ] Merge results using `HybridSearchService.merge_results()`
  - [ ] If `use_re_ranking` is True:
    - [ ] Take initial results (vector or hybrid)
    - [ ] Limit to `settings.rag.reranking_top_k` results
    - [ ] Call `RerankingService.rerank_chunks()` with LLM provider from ProviderFactory
  - [ ] Return final results with appropriate scores (similarity_score, bm25_score, rerank_score)
  - [ ] Ensure all business logic remains in use case layer [Source: architecture/coding-standards.md#critical-rules]

- [ ] Implement Redis Cache Service (AC: 5, 6) - NFR6
  - [ ] Create new service class `RedisCacheService` in `src/infrastructure/cache/redis_cache.py` [Source: architecture/source-tree.md#infrastructure-cache]
  - [ ] Implement `async def get(key: str) -> Optional[str]` method to retrieve cached value
  - [ ] Implement `async def set(key: str, value: str, ttl: int) -> None` method to store value with TTL
  - [ ] Implement `async def delete(key: str) -> None` method to invalidate cache entry
  - [ ] Implement `async def generate_cache_key(project_id: UUID, query_text: str, use_hybrid: bool, use_rerank: bool, top_k: int) -> str` method
  - [ ] Cache key format: `{prefix}{project_id}:{hash(query_text)}:{use_hybrid}:{use_rerank}:{top_k}` [Source: AC 6]
  - [ ] Use `hashlib.sha256` to hash query_text for consistent key generation
  - [ ] Handle Redis connection errors gracefully (log warning, continue without cache)
  - [ ] All Redis operations MUST use async redis client [Source: architecture/coding-standards.md#critical-rules]

- [ ] Integrate Redis Caching in QueryKnowledgeUseCase (AC: 5, 6)
  - [ ] Update `QueryKnowledgeUseCase` to inject `RedisCacheService` dependency
  - [ ] Before executing search, generate cache key and attempt to retrieve cached results
  - [ ] If cache hit: deserialize and return cached results (log cache hit for observability)
  - [ ] If cache miss: execute search logic (vector/hybrid/reranking), serialize results, and store in cache with TTL
  - [ ] Only cache successful queries (do not cache errors)
  - [ ] Serialize results as JSON using Pydantic's `.json()` method
  - [ ] Respect `settings.rag.cache_enabled` flag - skip caching if disabled
  - [ ] Add structured logging for cache operations (hit/miss/error) [Source: architecture/coding-standards.md NFR9]

- [ ] Update RAG API Endpoint (AC: 1)
  - [ ] Update POST `/api/v1/rag/query` route in `src/api/v1/routes/rag.py` to pass new request fields (`use_hybrid_search`, `use_re_ranking`) to QueryKnowledgeUseCase
  - [ ] Update response mapping to include new score fields (`bm25_score`, `rerank_score`) in KnowledgeItemResult
  - [ ] Ensure no business logic is added to API layer - only pass-through [Source: architecture/coding-standards.md#critical-rules]

- [ ] Unit Tests for Hybrid Search Service (AC: 5)
  - [ ] Create `tests/unit/application/services/test_hybrid_search_service.py`
  - [ ] Test RRF merge with vector-only results
  - [ ] Test RRF merge with keyword-only results
  - [ ] Test RRF merge with overlapping results (deduplication)
  - [ ] Test RRF merge with different weights
  - [ ] Test edge case: empty result lists
  - [ ] All tests MUST use AAA pattern and achieve 90%+ coverage [Source: architecture/test-strategy-and-standards.md]

- [ ] Unit Tests for Re-ranking Service (AC: 5)
  - [ ] Create `tests/unit/application/services/test_reranking_service.py`
  - [ ] Test successful re-ranking with valid LLM response
  - [ ] Test LLM failure handling (returns original order)
  - [ ] Test invalid JSON response handling
  - [ ] Test partial re-ranking (fewer chunks than requested)
  - [ ] Mock LLM provider using pytest-mock [Source: architecture/test-strategy-and-standards.md]
  - [ ] All tests MUST use AAA pattern and achieve 90%+ coverage [Source: architecture/test-strategy-and-standards.md]

- [ ] Unit Tests for Redis Cache Service (AC: 5, 7)
  - [ ] Create `tests/unit/infrastructure/cache/test_redis_cache.py`
  - [ ] Test cache key generation with different parameters
  - [ ] Test successful cache set and get operations
  - [ ] Test cache deletion
  - [ ] Test cache key hashing consistency (same input = same key)
  - [ ] Test Redis connection error handling (graceful degradation)
  - [ ] Mock Redis client using pytest-mock [Source: architecture/test-strategy-and-standards.md]
  - [ ] All tests MUST use AAA pattern and achieve 80%+ coverage [Source: architecture/test-strategy-and-standards.md]

- [ ] Integration Tests for Keyword Search (AC: 2)
  - [ ] Create `tests/integration/infrastructure/database/repositories/test_knowledge_repository_keyword_search.py`
  - [ ] Test keyword search returns correct results ordered by BM25 score
  - [ ] Test keyword search filters by project_id
  - [ ] Test keyword search respects top_k limit
  - [ ] Test keyword search returns empty list for project with no documents
  - [ ] Test BM25 scores are properly computed and ordered
  - [ ] Use Testcontainers for real PostgreSQL with full-text index [Source: architecture/test-strategy-and-standards.md]
  - [ ] Ensure test isolation with transaction rollback [Source: architecture/test-strategy-and-standards.md]

- [ ] Integration Tests for Updated Use Case (AC: 5, 7)
  - [ ] Update `tests/unit/application/use_cases/knowledge/test_query_knowledge.py` to add:
  - [ ] Test hybrid search enabled (combines vector + keyword)
  - [ ] Test re-ranking enabled (LLM provider called)
  - [ ] Test hybrid search + re-ranking combined
  - [ ] Test configuration defaults (use_hybrid_search=False, use_re_ranking=False)
  - [ ] Test cache hit scenario (cached results returned, no database query)
  - [ ] Test cache miss scenario (database queried, results cached)
  - [ ] Test cache disabled scenario (cache_enabled=False, no caching operations)
  - [ ] Mock HybridSearchService, RerankingService, and RedisCacheService [Source: architecture/test-strategy-and-standards.md]

- [ ] Integration Tests for Redis Caching (AC: 5, 6, 7)
  - [ ] Create `tests/integration/infrastructure/cache/test_redis_cache_integration.py`
  - [ ] Test full cache workflow with real Redis (Testcontainers)
  - [ ] Test cache TTL expiration (verify entry expires after TTL)
  - [ ] Test cache key uniqueness (different parameters = different keys)
  - [ ] Test concurrent cache access (multiple simultaneous queries)
  - [ ] Use Testcontainers for real Redis instance [Source: architecture/test-strategy-and-standards.md]
  - [ ] Ensure test isolation with cache flush between tests [Source: architecture/test-strategy-and-standards.md]

- [ ] E2E Tests for Advanced RAG Features (AC: 7)
  - [ ] Update `tests/e2e/api/v1/test_rag.py` to add:
  - [ ] Test POST `/api/v1/rag/query` with `use_hybrid_search=true` returns 200 with combined results
  - [ ] Test POST `/api/v1/rag/query` with `use_re_ranking=true` returns 200 with re-ranked results
  - [ ] Test POST `/api/v1/rag/query` with both flags enabled returns 200 with hybrid + re-ranked results
  - [ ] Test response schema includes new score fields (bm25_score, rerank_score)
  - [ ] Test default behavior (flags=false) still works (regression test)
  - [ ] Test cache hit: Make identical query twice, verify second request is faster (check logs for cache hit)
  - [ ] Test cache invalidation: Clear cache between different queries
  - [ ] Mock LLM provider to avoid external API calls [Source: architecture/test-strategy-and-standards.md]
  - [ ] All E2E tests MUST create their own test data and clean up [Source: architecture/test-strategy-and-standards.md]

## Dev Notes

### Previous Story Insights
[Source: docs/stories/3.1.story.md]

**From Story 3.1 (RAG Retrieval API - Core Query)**:
- Basic RAG query endpoint is operational: POST `/api/v1/rag/query` with JWT authentication
- Vector similarity search implemented using pgvector's cosine similarity operator (`<=>`)
- HNSW index provides O(log n) approximate search for fast vector retrieval
- QueryKnowledgeUseCase handles project validation, top_k enforcement, and embedding generation
- RBAC implemented: users can only query their own projects (owner_id field)
- All 20 tests passing (7 unit + 5 integration + 8 E2E)
- Schemas defined: RAGQueryRequest, RAGQueryResponse, KnowledgeItemResult
- Settings configured: RAGSettings with default_top_k=5, max_top_k=50

**Key Learnings**:
1. Use `get_fresh_pool()` in integration tests for proper test isolation
2. Mock embedding provider in E2E tests using `unittest.mock.patch` to avoid external dependencies
3. Use directionally different vectors (not just different magnitudes) for proper cosine similarity testing
4. Settings should be loaded via `load_settings()` call, not dependency injection in some cases

**Existing Infrastructure to Leverage**:
- Vector search already implemented: `KnowledgeRepository.vector_search()`
- LLM provider factory available: `ProviderFactory.get_embedding_provider()` and `ProviderFactory.get_llm_provider()`
- Project validation logic: Check project exists and user owns it
- Top-K enforcement: Respect max_top_k setting
- Error handling: Custom exceptions (ProjectNotFoundError, UnauthorizedAccessError)

### Data Models
[Source: architecture/database-schema.md, architecture/data-models.md]

**KnowledgeItem Entity** (from `src/domain/models/knowledge.py`):
- `id: UUID` - Primary key
- `document_id: UUID` - Foreign key to documents table
- `chunk_text: str` - Text content of the chunk (will be used for full-text search)
- `embedding: List[float]` - Vector embedding (1536 dimensions for text-embedding-3-small)
- `metadata: Optional[Dict]` - JSON metadata
- `created_at: datetime`
- `updated_at: datetime`

**Full-Text Search Index** (new):
- Table: `knowledge_items`
- Column: `chunk_text`
- Index Type: GIN (Generalized Inverted Index)
- Function: `to_tsvector('english', chunk_text)`
- Purpose: Fast BM25-like keyword search

### API Specifications
[Source: architecture/rest-api-spec.md]

**Updated POST `/api/v1/rag/query`**:

**Request Schema**:
```json
{
  "project_id": "uuid",
  "query_text": "string (1-10,000 chars)",
  "top_k": 5,  // optional, default from settings
  "use_hybrid_search": false,  // NEW - optional, default false
  "use_re_ranking": false  // NEW - optional, default false
}
```

**Response Schema**:
```json
{
  "results": [
    {
      "id": "uuid",
      "chunk_text": "string",
      "similarity_score": 0.95,  // from vector search
      "bm25_score": 0.87,  // NEW - optional, from keyword search
      "rerank_score": 0.92,  // NEW - optional, from LLM re-ranking
      "metadata": {},
      "document_id": "uuid"
    }
  ],
  "query_id": "uuid",
  "total_results": 5
}
```

**Authentication**: JWT Bearer token (required)
**Status Codes**: 200 (success), 401 (unauthorized), 403 (forbidden), 404 (project not found), 422 (validation error)

### Component Specifications
[Source: architecture/source-tree.md]

**New Services**:
1. `HybridSearchService` - Located at `src/application/services/hybrid_search_service.py`
   - Merges vector and keyword search results using Reciprocal Rank Fusion
   - Pure business logic, no external dependencies
   
2. `RerankingService` - Located at `src/application/services/reranking_service.py`
   - Uses LLM provider to re-rank search results
   - Depends on ILLMProvider interface (from domain layer)

3. `RedisCacheService` - Located at `src/infrastructure/cache/redis_cache.py` (NFR6)
   - Handles Redis caching operations for RAG query results
   - Implements get/set/delete operations with TTL support
   - Generates consistent cache keys based on query parameters
   - Depends on ILLMProvider interface (from domain layer)

**Updated Components**:
1. `KnowledgeRepository` - Add `keyword_search()` method
2. `QueryKnowledgeUseCase` - Add hybrid search, re-ranking, and Redis caching logic
3. `RAGQueryRequest` schema - Add boolean flags
4. `KnowledgeItemResult` schema - Add score fields

### File Locations
[Source: architecture/source-tree.md]

**New Files**:
- `src/application/services/hybrid_search_service.py` - Hybrid search merge logic
- `src/application/services/reranking_service.py` - LLM re-ranking logic
- `src/infrastructure/cache/redis_cache.py` - Redis caching service (NFR6)
- `tests/unit/application/services/test_hybrid_search_service.py` - Unit tests
- `tests/unit/application/services/test_reranking_service.py` - Unit tests
- `tests/unit/infrastructure/cache/test_redis_cache.py` - Unit tests for cache service
- `tests/integration/infrastructure/cache/test_redis_cache_integration.py` - Integration tests with real Redis
- `tests/integration/infrastructure/database/repositories/test_knowledge_repository_keyword_search.py` - Integration tests
- `migration/versions/{timestamp}_add_fulltext_index.py` - Alembic migration

**Modified Files**:
- `src/api/v1/schemas/rag.py` - Update request/response schemas
- `src/shared/config/settings.py` - Add advanced RAG and caching settings
- `src/domain/models/knowledge.py` - Add keyword_search() to interface
- `src/infrastructure/database/repositories/knowledge_repository.py` - Implement keyword_search()
- `src/application/use_cases/knowledge/query_knowledge.py` - Add hybrid/reranking/caching logic
- `src/api/v1/routes/rag.py` - Pass new parameters to use case
- `tests/unit/application/use_cases/knowledge/test_query_knowledge.py` - Add new test cases
- `tests/e2e/api/v1/test_rag.py` - Add new E2E tests

### Testing Requirements
[Source: architecture/test-strategy-and-standards.md]

**Test File Locations**:
- Unit tests: `tests/unit/application/services/` for new services, `tests/unit/application/use_cases/knowledge/` for updated use case
- Integration tests: `tests/integration/infrastructure/database/repositories/` for keyword search
- E2E tests: `tests/e2e/api/v1/` for API endpoint

**Coverage Requirements**:
- Application layer (services, use cases): 90%+
- Infrastructure layer (repositories): 80%+
- Overall story: 85%+

**Test Patterns**:
- Follow AAA (Arrange, Act, Assert) pattern
- Use pytest fixtures for dependencies
- Mock external dependencies (LLM provider) in unit and E2E tests
- Use Testcontainers for real PostgreSQL in integration tests
- All integration tests MUST run in transactions that rollback

**Critical Test Cases**:
1. **Hybrid Search**:
   - Vector search only (use_hybrid_search=false)
   - Keyword search only (mock scenario)
   - Combined search (use_hybrid_search=true) with result merging
   - Overlapping results deduplication
   - Empty results handling

2. **Re-ranking**:
   - Successful re-ranking with valid LLM response
   - LLM failure handling (fallback to original order)
   - Invalid JSON response handling
   - Partial re-ranking (fewer results than expected)

3. **Combined**:
   - Hybrid search + re-ranking together
   - Configuration defaults respected
   - All score fields populated correctly in response

4. **Edge Cases**:
   - No documents in project (empty results)
   - Query matches vector but not keyword (and vice versa)
   - Re-ranking with single result

### Technical Constraints
[Source: architecture/coding-standards.md, PRD NFR1-NFR9]

**Clean Architecture Rules**:
- HybridSearchService and RerankingService MUST NOT import from api or infrastructure layers
- Services can only depend on domain interfaces (ILLMProvider, domain models)
- API layer only passes parameters to use case, no business logic

**PostgreSQL Full-Text Search**:
- Use `to_tsvector('english', chunk_text)` for text vectorization
- Use `to_tsquery('english', query)` for query parsing
- Use `ts_rank_cd()` for BM25-like scoring
- GIN index required for performance: `CREATE INDEX ... USING gin(to_tsvector(...))`

**Reciprocal Rank Fusion (RRF) Algorithm**:
- Formula: `score = w1 * (1 / (rank1 + k)) + w2 * (1 / (rank2 + k))` where k=60 (standard)
- Weights from settings: `hybrid_search_weight_vector` and `hybrid_search_weight_bm25`
- Deduplication: Same KnowledgeItem from both sources â†’ keep highest combined score

**LLM Re-ranking**:
- Use `ILLMProvider.generate_completion()` method (from Story 2.2)
- Prompt must be clear: "Rank by relevance, return JSON array of indices"
- Error handling: LLM failures should not break the query (return original order with log warning)
- Model from settings: `reranking_model` (default: gpt-4o-mini)

**Redis Caching** (NFR6):
- Cache key format: `{prefix}{project_id}:{sha256(query_text)}:{use_hybrid}:{use_rerank}:{top_k}`
- Use `hashlib.sha256` for query text hashing (consistent, fast, collision-resistant)
- Cache TTL: Default 3600 seconds (1 hour), configurable via settings
- Graceful degradation: Redis connection errors should not break queries (log warning, continue without cache)
- Only cache successful queries (HTTP 200 responses)
- Serialize/deserialize using Pydantic `.json()` and `.parse_raw()` methods
- Async Redis client required: Use `redis.asyncio` library
- Cache invalidation: Manual deletion via cache service or TTL expiration

**Performance Considerations**:
- GIN index on chunk_text for fast full-text search
- HNSW index on embeddings for fast vector search (from Story 3.1)
- Redis caching reduces database load for repeated queries (NFR6)
- Limit re-ranking to top_k chunks (default: 10) to reduce LLM cost
- Async/await for all I/O operations
- Parameterized queries for SQL injection prevention

**Security**:
- All queries must filter by project_id
- JWT authentication required (existing from Story 3.1)
- RBAC: Verify user owns project (existing from Story 3.1)
- Input validation via Pydantic (query_text length, boolean flags)

### Testing

#### Testing Standards
[Source: architecture/test-strategy-and-standards.md]

**Test File Locations**:
- Unit tests: `tests/unit/` mirroring `src/` structure
- Integration tests: `tests/integration/` mirroring infrastructure structure
- E2E tests: `tests/e2e/api/v1/`

**Testing Framework**: pytest with pytest-mock for mocking

**Coverage Requirements**:
- Application layer (services, use cases): 90%+
- Infrastructure layer (repositories): 80%+
- Overall story: 85%+

**Test Patterns**:
- Follow AAA (Arrange, Act, Assert) pattern
- Use pytest fixtures for dependencies
- Use factory_boy for test data creation
- All integration tests MUST use Testcontainers for real PostgreSQL/pgvector
- All tests MUST run in transactions that rollback for isolation
- E2E tests create their own data and clean up

**Mocking Strategy**:
- Unit tests: Mock ALL external dependencies (repositories, LLM provider)
- Integration tests: Use real database (Testcontainers), stub LLM APIs (pytest-httpx or unittest.mock)
- E2E tests: Test against running FastAPI app, mock LLM provider to avoid external API calls

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-09 | 1.0 | Initial story draft created | Bob (Scrum Master) |
| 2025-11-09 | 1.1 | Added Redis caching implementation (NFR6) | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be filled by Dev Agent)

### Debug Log References
(To be filled by Dev Agent)

### Completion Notes List
(To be filled by Dev Agent)

### File List
(To be filled by Dev Agent)

## QA Results

(To be filled by QA Agent)
