# Story 2.5: Knowledge Ingestion - File Upload Pipeline

## Status

**Draft**

## Story

**As a** AI Agent Developer,
**I want** to upload a file (MD, PDF, DOCX, HTML) to an endpoint,
**so that** it is automatically processed, chunked, embedded, and stored as KnowledgeItems.

## Acceptance Criteria

1. A new endpoint POST `/api/v1/knowledge/upload` is created that accepts a file upload (FR6).
2. The endpoint creates a new Document (using logic from 2.3) to represent the file.
3. The system correctly extracts text content from the supported file types (MD, PDF, DOCX, HTML) (FR4).
4. The extracted text is passed to a text chunker (e.g., semantic chunking).
5. The text chunks are passed to the embedding provider (from Story 2.2) to get vectors.
6. Each chunk, its vector, and metadata are saved as a new KnowledgeItem in the database, linked to the Document (FR8).
7. This process is asynchronous (e.g., using FastAPI's BackgroundTasks) to avoid blocking the API response.

## Tasks / Subtasks

- [ ] Create Pydantic request/response schemas for knowledge upload (AC: 1, 7)
  - [ ] Create KnowledgeUploadRequest schema in `src/api/v1/schemas/knowledge.py` with fields: project_id (UUID), file (UploadFile) [Source: architecture/rest-api-spec.md]
  - [ ] Create KnowledgeUploadResponse schema with fields: document_id (UUID), status (str - "processing"), message (str)
  - [ ] Create KnowledgeItemResponse schema matching KnowledgeItem entity (id, document_id, chunk_text, metadata) [Source: architecture/data-models.md#knowledgeitem]
  - [ ] All schemas MUST use type hints and inherit from Pydantic BaseModel [Source: architecture/coding-standards.md]

- [ ] Create text extraction service for multiple file formats (AC: 3)
  - [ ] Create TextExtractor service in `src/application/services/text_extractor.py` [Source: architecture/source-tree.md#application-layer]
  - [ ] Implement extract_markdown() method for .md files (return raw text)
  - [ ] Implement extract_pdf() method using PyPDF2 or pdfplumber library [Source: architecture/tech-stack.md]
  - [ ] Implement extract_docx() method using python-docx library
  - [ ] Implement extract_html() method using BeautifulSoup to extract text content
  - [ ] Add file type detection based on file extension or MIME type
  - [ ] Add error handling for corrupted or unsupported files (raise TextExtractionError)
  - [ ] All extraction methods MUST be async [Source: architecture/coding-standards.md#async]

- [ ] Create text chunking service (AC: 4)
  - [ ] Create TextChunker service in `src/application/services/text_chunker.py`
  - [ ] Implement semantic_chunk() method with configurable chunk_size (default 512 tokens) and overlap (default 50 tokens)
  - [ ] Use tiktoken or similar library for token counting (based on embedding model tokenizer)
  - [ ] Add metadata to each chunk: chunk_index, start_char, end_char, token_count
  - [ ] Preserve sentence boundaries when possible (don't split mid-sentence)
  - [ ] All chunking methods MUST be async

- [ ] Create knowledge ingestion use case (AC: 2, 4, 5, 6, 7)
  - [ ] Create IngestKnowledgeUseCase in `src/application/use_cases/ingest_knowledge.py` [Source: architecture/source-tree.md]
  - [ ] Inject dependencies: DocumentRepository, KnowledgeRepository, TextExtractor, TextChunker, EmbeddingService
  - [ ] Implement async execute() method accepting: file (UploadFile), project_id (UUID), user_id (UUID)
  - [ ] Step 1: Create Document entity with name=file.filename, type=detected_type, version="1.0.0", project_id, content_hash=hash(file_content) [Source: architecture/data-models.md#document]
  - [ ] Step 2: Call TextExtractor.extract(file) to get raw text content
  - [ ] Step 3: Call TextChunker.semantic_chunk(text) to get text chunks
  - [ ] Step 4: For each chunk, call EmbeddingService.embed_text(chunk_text) to get vector [Source: architecture/core-workflows.md#workflow-1]
  - [ ] Step 5: For each chunk, create KnowledgeItem entity with document_id, chunk_text, embedding, metadata [Source: architecture/data-models.md#knowledgeitem]
  - [ ] Step 6: Batch save all KnowledgeItems to KnowledgeRepository
  - [ ] Step 7: Update Document status to "completed"
  - [ ] Add error handling: catch TextExtractionError, EmbeddingError, DatabaseError
  - [ ] Follow Clean Architecture: NO direct database access, only through repositories [Source: architecture/coding-standards.md#critical-rules]

- [ ] Implement POST /api/v1/knowledge/upload endpoint (AC: 1, 7)
  - [ ] Create `upload_knowledge` route in `src/api/v1/routes/knowledge.py` [Source: architecture/source-tree.md#api-layer]
  - [ ] Protect with `Depends(get_current_user)` from Story 1.4 [Source: architecture/security.md#authentication-authorization]
  - [ ] Accept file upload using FastAPI's UploadFile and project_id as form field
  - [ ] Validate file extension is in allowed list: ['.md', '.pdf', '.docx', '.html']
  - [ ] Validate file size limit (e.g., max 10MB, configurable via settings.py)
  - [ ] Create DocumentRepository and KnowledgeRepository dependencies
  - [ ] Call IngestKnowledgeUseCase.execute() in BackgroundTasks to run async [Source: architecture/core-workflows.md#workflow-1]
  - [ ] Return 202 Accepted immediately with KnowledgeUploadResponse {"document_id": "...", "status": "processing"} [Source: architecture/rest-api-spec.md]
  - [ ] Handle errors: 401 Unauthorized, 413 File Too Large, 422 Validation Error (unsupported file type)
  - [ ] Follow Clean Architecture: NO business logic in route, only validation and response formatting [Source: architecture/coding-standards.md#critical-rules]

- [ ] Add required dependencies to pyproject.toml (AC: 3)
  - [ ] Add PyPDF2 or pdfplumber for PDF extraction
  - [ ] Add python-docx for DOCX extraction
  - [ ] Add beautifulsoup4 for HTML extraction
  - [ ] Add tiktoken for token counting
  - [ ] Run `poetry lock` and `poetry install` to update dependencies

- [ ] Create E2E tests for knowledge upload endpoint (AC: 1-7)
  - [ ] Create test file `tests/e2e/api/v1/test_knowledge.py` [Source: architecture/test-strategy-and-standards.md]
  - [ ] Test 1: test_upload_knowledge_markdown_success - upload .md file, verify 202 response, check document created
  - [ ] Test 2: test_upload_knowledge_pdf_success - upload .pdf file, verify processing
  - [ ] Test 3: test_upload_knowledge_docx_success - upload .docx file, verify processing
  - [ ] Test 4: test_upload_knowledge_html_success - upload .html file, verify processing
  - [ ] Test 5: test_upload_knowledge_unauthorized - verify 401 without auth token
  - [ ] Test 6: test_upload_knowledge_invalid_file_type - verify 422 for unsupported file type
  - [ ] Test 7: test_upload_knowledge_file_too_large - verify 413 for file exceeding size limit
  - [ ] Test 8: test_upload_knowledge_creates_knowledge_items - wait for background task, verify KnowledgeItems created in DB
  - [ ] Test 9: test_upload_knowledge_chunking - verify chunks are properly created with metadata
  - [ ] Test 10: test_upload_knowledge_embedding - verify embeddings are generated and stored
  - [ ] Use pytest fixtures for test file creation and cleanup
  - [ ] Use httpx.AsyncClient for real HTTP requests [Source: architecture/test-strategy-and-standards.md#e2e-tests]

- [ ] Create unit tests for TextExtractor service
  - [ ] Create test file `tests/unit/application/services/test_text_extractor.py`
  - [ ] Test extract_markdown() with sample .md file
  - [ ] Test extract_pdf() with sample .pdf file
  - [ ] Test extract_docx() with sample .docx file
  - [ ] Test extract_html() with sample .html file
  - [ ] Test error handling for corrupted files
  - [ ] Test file type detection logic
  - [ ] Follow AAA pattern (Arrange, Act, Assert) [Source: architecture/test-strategy-and-standards.md#unit-tests]

- [ ] Create unit tests for TextChunker service
  - [ ] Create test file `tests/unit/application/services/test_text_chunker.py`
  - [ ] Test semantic_chunk() with various text lengths
  - [ ] Test chunk size and overlap configuration
  - [ ] Test token counting accuracy
  - [ ] Test sentence boundary preservation
  - [ ] Test metadata generation (chunk_index, start_char, end_char)
  - [ ] Test edge cases: empty text, single sentence, very long text

## Dev Notes

### Previous Story Insights

From Story 2.4 (Task Management API):
- Schema pattern: Separate Create, Update, Response, ListResponse schemas with full type hints
- Validation: Use Pydantic Field with constraints (min_length, max_length, custom validators)
- Error handling: Custom exceptions converted by FastAPI middleware
- Response codes: 201 for creation, 202 for async processing, 401 unauthorized, 422 validation error
- Testing: Comprehensive E2E tests covering happy path, auth, validation, error cases
- Clean Architecture: NO business logic in routes, only validation and calling use cases

From Story 2.3 (Document Management API):
- Document entity already defined with fields: project_id, name, type, version, content_hash
- DocumentRepository already implemented with create(), get_by_id(), update() methods
- Document status tracking: "draft", "processing", "completed", "failed"
- Semantic versioning support: version field as string (e.g., "1.0.0")

From Story 2.2 (LLM & Embedding Provider Factory):
- ILLMProvider interface already defined with embed_text() method
- EmbeddingService abstraction available via ProviderFactory
- Concrete providers: OpenAIProvider, AnthropicProvider, OllamaProvider, OpenRouterProvider
- Configuration in settings.py: LLM_PROVIDER, EMBEDDING_PROVIDER, API keys
- Factory integrated into FastAPI dependency injection

From Story 2.1 (Core Domain Models):
- Document entity defined in `src/domain/models/document.py`
- KnowledgeItem entity defined in `src/domain/models/knowledge.py` with fields: id, document_id, chunk_text, embedding (Vector), metadata (JSON)
- IDocumentRepository and IKnowledgeRepository interfaces defined
- Concrete PostgreSQL implementations exist in `src/infrastructure/database/repositories/`
- Database schema: documents and knowledge_items tables already created via Alembic migration
- Vector column on knowledge_items uses pgvector's Vector type

### Technical Architecture

**Asynchronous Processing Pattern** [Source: architecture/core-workflows.md#workflow-1]:
```
1. Client uploads file → API endpoint
2. API creates Document record, returns 202 Accepted immediately
3. BackgroundTasks runs ingestion pipeline asynchronously:
   - Extract text from file
   - Chunk text into semantic segments
   - Generate embeddings for each chunk
   - Save KnowledgeItems to database
   - Update Document status to "completed"
4. Client can poll GET /api/v1/documents/{id} to check status
```

**File Upload Flow**:
1. FastAPI UploadFile receives multipart/form-data
2. Validate file extension and size
3. Read file content into memory (or stream for large files)
4. Pass to TextExtractor service
5. Background task handles processing

**Text Extraction Strategy** [Source: PRD FR4]:
- Markdown (.md): Read raw text directly
- PDF (.pdf): Use PyPDF2 or pdfplumber to extract text from all pages
- DOCX (.docx): Use python-docx to extract paragraphs and tables
- HTML (.html): Use BeautifulSoup to parse and extract text content (strip tags)

**Text Chunking Strategy**:
- Use semantic chunking with configurable chunk size (default 512 tokens)
- Add overlap between chunks (default 50 tokens) to preserve context
- Preserve sentence boundaries (don't split mid-sentence)
- Generate metadata for each chunk: chunk_index, start_char, end_char, token_count
- Use tiktoken library for token counting (matches OpenAI tokenizer)

**Embedding Generation** [Source: architecture/core-workflows.md]:
- Call EmbeddingService.embed_text(chunk_text) for each chunk
- EmbeddingService uses configured provider (OpenAI, Anthropic, Ollama, etc.)
- Returns vector embedding as list[float] or numpy array
- Store embedding in KnowledgeItem.embedding field (pgvector Vector type)

**Error Handling**:
- TextExtractionError: File is corrupted or unsupported format
- EmbeddingError: External LLM API failure (retry logic recommended)
- DatabaseError: Failed to save KnowledgeItems
- All errors logged with structured logging [Source: architecture/coding-standards.md]
- Document status updated to "failed" on error

### Database Schema

**knowledge_items table** (already created in Story 2.1):
```sql
CREATE TABLE knowledge_items (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    chunk_text TEXT NOT NULL,
    embedding vector(1536),  -- Dimension depends on embedding model
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT now()
);
CREATE INDEX idx_knowledge_items_document_id ON knowledge_items(document_id);
CREATE INDEX idx_knowledge_items_embedding ON knowledge_items USING ivfflat (embedding vector_cosine_ops);
```

**Important Notes**:
- CASCADE delete: Deleting a document deletes all its knowledge items
- embedding dimension (1536) is for OpenAI text-embedding-3-small; adjust for other models
- ivfflat index for fast vector similarity search
- metadata stored as JSONB for flexible chunk information

### KnowledgeItem Domain Model

**KnowledgeItem Entity** [Source: architecture/data-models.md#knowledgeitem]:
```python
@dataclass(slots=True)
class KnowledgeItem:
    id: UUID
    document_id: UUID
    chunk_text: str
    embedding: list[float]  # Vector representation
    metadata: dict[str, Any]  # chunk_index, start_char, end_char, token_count
    created_at: datetime
```

**Metadata Structure**:
```json
{
  "chunk_index": 0,
  "start_char": 0,
  "end_char": 512,
  "token_count": 128,
  "source_page": 1  // Optional, for PDFs
}
```

### IKnowledgeRepository Interface

**Repository Methods** (already defined in Story 2.1):
```python
class IKnowledgeRepository(ABC):
    async def create(knowledge_item: KnowledgeItem) -> KnowledgeItem
    async def create_batch(knowledge_items: list[KnowledgeItem]) -> list[KnowledgeItem]
    async def get_by_id(knowledge_item_id: UUID) -> KnowledgeItem | None
    async def get_by_document(document_id: UUID) -> list[KnowledgeItem]
    async def search_similar(embedding: list[float], limit: int) -> list[KnowledgeItem]
    async def delete(knowledge_item_id: UUID) -> None
```

**Key Method for This Story**: `create_batch()` - used to efficiently save all chunks at once

### API Request/Response Schemas

**File Upload Request** (multipart/form-data):
```python
# Route signature
@router.post("/upload")
async def upload_knowledge(
    file: UploadFile = File(...),
    project_id: UUID = Form(...),
    current_user: User = Depends(get_current_user),
    background_tasks: BackgroundTasks,
) -> KnowledgeUploadResponse:
```

**KnowledgeUploadResponse**:
```python
class KnowledgeUploadResponse(BaseModel):
    document_id: UUID
    status: str  # "processing"
    message: str  # "File upload successful. Processing in background."
```

### Configuration Settings

**New Settings in settings.py**:
```python
# File Upload Configuration
MAX_FILE_SIZE_MB: int = 10  # Maximum file upload size
ALLOWED_FILE_EXTENSIONS: list[str] = ['.md', '.pdf', '.docx', '.html']

# Text Chunking Configuration
CHUNK_SIZE_TOKENS: int = 512  # Default chunk size
CHUNK_OVERLAP_TOKENS: int = 50  # Overlap between chunks
PRESERVE_SENTENCE_BOUNDARIES: bool = True

# Embedding Configuration (from Story 2.2)
EMBEDDING_PROVIDER: str = "openai"  # or "anthropic", "ollama", "openrouter"
EMBEDDING_MODEL: str = "text-embedding-3-small"
EMBEDDING_DIMENSION: int = 1536  # Depends on model
```

### Testing

**E2E Testing Strategy** [Source: architecture/test-strategy-and-standards.md]:
- Test file: `tests/e2e/api/v1/test_knowledge.py`
- Use httpx.AsyncClient for real HTTP requests
- Test all supported file formats (MD, PDF, DOCX, HTML)
- Test authentication (401 without token)
- Test validation (422 for invalid file type, 413 for too large)
- Test asynchronous processing (verify 202 response, then check DB for KnowledgeItems)
- Use pytest fixtures for test file creation and cleanup
- Wait for background tasks to complete using asyncio.sleep() or polling

**Unit Testing Strategy**:
- Test TextExtractor: All extraction methods with sample files, error handling
- Test TextChunker: Various text lengths, chunk configurations, metadata generation
- Follow AAA pattern (Arrange, Act, Assert)
- Mock external dependencies (embedding service, repositories)
- Coverage goal: 90%+ for application layer [Source: architecture/test-strategy-and-standards.md]

**Test Sample Files**:
- Create fixtures in `tests/fixtures/sample_files/`:
  - sample.md: Markdown file with 1000+ words
  - sample.pdf: PDF file with multiple pages
  - sample.docx: Word document with formatting
  - sample.html: HTML file with tags and content
  - corrupted.pdf: Corrupted file for error testing

### Dependencies to Add

**Python Libraries** [Source: architecture/tech-stack.md]:
```toml
[tool.poetry.dependencies]
PyPDF2 = "^3.0.0"  # or pdfplumber = "^0.10.0"
python-docx = "^1.1.0"
beautifulsoup4 = "^4.12.0"
tiktoken = "^0.5.0"  # Token counting for OpenAI models
```

### Clean Architecture Layers

**API Layer** (`src/api/v1/routes/knowledge.py`):
- Route: upload_knowledge()
- Responsibility: Validate file, call use case, return 202 response
- NO business logic, NO direct repository access

**Application Layer** (`src/application/use_cases/ingest_knowledge.py`):
- Use Case: IngestKnowledgeUseCase
- Services: TextExtractor, TextChunker
- Responsibility: Orchestrate ingestion pipeline (extract → chunk → embed → save)
- Inject all dependencies via constructor
- NO direct database access (use repositories)

**Infrastructure Layer** (`src/infrastructure/database/repositories/`):
- KnowledgeRepository: Implements IKnowledgeRepository
- DocumentRepository: Implements IDocumentRepository
- Responsibility: Database operations, SQL queries, pgvector operations

**Domain Layer** (`src/domain/models/`):
- Entities: Document, KnowledgeItem (already defined)
- NO changes needed for this story

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-08 | 0.1 | Initial draft created following create-next-story task | Scrum Master (Bob) |

## Dev Agent Record

### Agent Model Used

_To be filled by development agent_

### Debug Log References

_To be filled by development agent_

### Completion Notes List

_To be filled by development agent_

### File List

_To be filled by development agent_

## QA Results

_To be filled by QA agent after implementation_
