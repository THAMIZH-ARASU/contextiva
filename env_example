# Application
APP_ENV=local
APP_HOST=0.0.0.0
APP_PORT=8000

# Database (used by app and Alembic migrations)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=contextiva
POSTGRES_USER=your-user
POSTGRES_PASSWORD=your-password

# Redis Cache
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# LLM Provider Configuration
# Options: openai, anthropic, ollama, openrouter
LLM_PROVIDER=openai
LLM_EMBEDDING_PROVIDER=openai

# OpenAI Configuration (required if using openai provider)
LLM_OPENAI_API_KEY=sk-...
LLM_DEFAULT_LLM_MODEL=gpt-4o-mini
LLM_DEFAULT_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic Configuration (optional, only if using anthropic provider)
# Note: Anthropic does not provide embeddings, use openai for embeddings
LLM_ANTHROPIC_API_KEY=sk-ant-...

# Ollama Configuration (optional, only if using ollama for local models)
# Ollama runs locally and does not require authentication
LLM_OLLAMA_BASE_URL=http://localhost:11434

# OpenRouter Configuration (optional, only if using openrouter for 100+ models)
# Note: OpenRouter does not provide embeddings directly
LLM_OPENROUTER_API_KEY=sk-or-...

# Legacy LLM Configuration (deprecated, use LLM_* variables above)
# LLM_API_KEY=sk-...
# LLM_MODEL=gpt-4o-mini
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=2000

# Embeddings (deprecated, use LLM_EMBEDDING_PROVIDER and LLM_DEFAULT_EMBEDDING_MODEL above)
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small
# EMBEDDING_DIMENSIONS=1536
# EMBEDDING_USE_CONTEXTUAL=false

# RAG Configuration
RAG_CHUNK_SIZE=5000
RAG_CHUNK_OVERLAP=200
RAG_MATCH_COUNT=5
RAG_SIMILARITY_THRESHOLD=0.05
RAG_USE_HYBRID_SEARCH=false
RAG_USE_RERANKING=false
RAG_USE_AGENTIC=false

# Cache (Optional)
CACHE_ENABLED=true
CACHE_REDIS_URL=redis://localhost:6379
CACHE_TTL=3600

# Observability
OBS_LOG_LEVEL=INFO
OBS_LOGFIRE_ENABLED=false
OBS_LOGFIRE_TOKEN=your-token

# Security & Authentication
JWT_SECRET=change-this-to-a-secure-random-secret-key
JWT_ALGORITHM=HS256
JWT_EXPIRES_MINUTES=30

# CORS (optional)
API_CORS_ORIGINS=["*"]